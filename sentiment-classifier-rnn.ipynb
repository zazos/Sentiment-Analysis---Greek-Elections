{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38cad674",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-24T14:13:49.386218Z",
     "iopub.status.busy": "2024-01-24T14:13:49.385784Z",
     "iopub.status.idle": "2024-01-24T14:13:49.854770Z",
     "shell.execute_reply": "2024-01-24T14:13:49.853513Z"
    },
    "papermill": {
     "duration": 0.482561,
     "end_time": "2024-01-24T14:13:49.857526",
     "exception": false,
     "start_time": "2024-01-24T14:13:49.374965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ys19-2023-assignment-3/sample_submission.csv\n",
      "/kaggle/input/ys19-2023-assignment-3/test_set.csv\n",
      "/kaggle/input/ys19-2023-assignment-3/valid_set.csv\n",
      "/kaggle/input/ys19-2023-assignment-3/train_set.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f86f0",
   "metadata": {
    "papermill": {
     "duration": 0.008171,
     "end_time": "2024-01-24T14:13:49.875437",
     "exception": false,
     "start_time": "2024-01-24T14:13:49.867266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef314f3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:13:49.896127Z",
     "iopub.status.busy": "2024-01-24T14:13:49.895512Z",
     "iopub.status.idle": "2024-01-24T14:13:57.425640Z",
     "shell.execute_reply": "2024-01-24T14:13:57.424213Z"
    },
    "papermill": {
     "duration": 7.544322,
     "end_time": "2024-01-24T14:13:57.429306",
     "exception": false,
     "start_time": "2024-01-24T14:13:49.884984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags_mentions(text):\n",
    "    hashtags = re.findall(r'\\#\\w+', text)\n",
    "    for hashtag in hashtags:\n",
    "        split_words = hashtag[1:].split('_')\n",
    "        text = text.replace(hashtag, ' '.join(split_words))\n",
    "    text = re.sub(r'\\@\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub(r'[.,!?:;/()|&]+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r'[\\\"«»-]', '', text)\n",
    "    return text\n",
    "\n",
    "def strip_accents_and_lowercase(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn').lower()\n",
    "\n",
    "def preprocess_tweets(text, stopwords=False):\n",
    "    text = remove_urls(text)\n",
    "    text = remove_hashtags_mentions(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = strip_accents_and_lowercase(text)\n",
    "    if stopwords:\n",
    "        tokens = word_tokenize(text)\n",
    "        custom_greek_stopwords = pd.read_csv('../input/greek-stopwords-custom/custom_greek_stopwords_lowercase.csv',\n",
    "                                                 delimiter='\\t')\n",
    "        greek_stopwords = set(custom_greek_stopwords['greek_stopwords'].values.flatten().tolist())\n",
    "        tokens = [token for token in tokens if token not in greek_stopwords]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "\n",
    "train_data = pd.read_csv('../input/ys19-2023-assignment-3/train_set.csv')\n",
    "valid_data = pd.read_csv('../input/ys19-2023-assignment-3/valid_set.csv')\n",
    "test_data = pd.read_csv('../input/ys19-2023-assignment-3/test_set.csv')\n",
    "\n",
    "train_data['Text'] = train_data['Text'].apply(preprocess_tweets)\n",
    "valid_data['Text'] = valid_data['Text'].apply(preprocess_tweets)\n",
    "test_data['Text'] = test_data['Text'].apply(preprocess_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9ec7a",
   "metadata": {
    "papermill": {
     "duration": 0.007867,
     "end_time": "2024-01-24T14:13:57.445564",
     "exception": false,
     "start_time": "2024-01-24T14:13:57.437697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3183db2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:13:57.465375Z",
     "iopub.status.busy": "2024-01-24T14:13:57.464893Z",
     "iopub.status.idle": "2024-01-24T14:15:30.851780Z",
     "shell.execute_reply": "2024-01-24T14:15:30.850130Z"
    },
    "papermill": {
     "duration": 93.400356,
     "end_time": "2024-01-24T14:15:30.855004",
     "exception": false,
     "start_time": "2024-01-24T14:13:57.454648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-24 14:14:14--  http://vectors.nlpl.eu/repository/20/46.zip\r\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\r\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 899316822 (858M) [application/zip]\r\n",
      "Saving to: '46.zip'\r\n",
      "\r\n",
      "46.zip              100%[===================>] 857.66M  24.5MB/s    in 36s     \r\n",
      "\r\n",
      "2024-01-24 14:14:50 (23.8 MB/s) - '46.zip' saved [899316822/899316822]\r\n",
      "\r\n",
      "Archive:  46.zip\r\n",
      "  inflating: LIST                    \r\n",
      "  inflating: meta.json               \r\n",
      "  inflating: model.bin               \r\n",
      "  inflating: model.txt               \r\n",
      "  inflating: README                  \r\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "!wget http://vectors.nlpl.eu/repository/20/46.zip\n",
    "!unzip -o 46.zip\n",
    "\n",
    "word2vec_greek_model = KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
    "# model's vector size is 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb2ff0",
   "metadata": {
    "papermill": {
     "duration": 0.023321,
     "end_time": "2024-01-24T14:15:30.902252",
     "exception": false,
     "start_time": "2024-01-24T14:15:30.878931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conver Tweet to padded Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5314b186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:15:30.954606Z",
     "iopub.status.busy": "2024-01-24T14:15:30.954116Z",
     "iopub.status.idle": "2024-01-24T14:15:35.597290Z",
     "shell.execute_reply": "2024-01-24T14:15:35.595730Z"
    },
    "papermill": {
     "duration": 4.672346,
     "end_time": "2024-01-24T14:15:35.600337",
     "exception": false,
     "start_time": "2024-01-24T14:15:30.927991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def tweet_to_embeddings(tweet, model):\n",
    "    embeddings = []\n",
    "    for word in tweet.split():\n",
    "        if word in model:\n",
    "            embedding = model[word]\n",
    "            embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "def padding_tweets(tweets, model, embedding_size=100):\n",
    "    padded_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tweet_embedding = tweet_to_embeddings(tweet, model)\n",
    "        if not tweet_embedding: # no words in tweet are in the model\n",
    "            tweet_embedding = [np.zeros(embedding_size)]\n",
    "        padded_tweets.append(torch.tensor(tweet_embedding)) # convert to tensor\n",
    "    \n",
    "    return pad_sequence(padded_tweets, batch_first=False, padding_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5ec77",
   "metadata": {
    "papermill": {
     "duration": 0.029745,
     "end_time": "2024-01-24T14:15:35.655237",
     "exception": false,
     "start_time": "2024-01-24T14:15:35.625492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Padded Embeddings and Sentiments for train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1192df87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:15:35.711214Z",
     "iopub.status.busy": "2024-01-24T14:15:35.710548Z",
     "iopub.status.idle": "2024-01-24T14:16:08.726263Z",
     "shell.execute_reply": "2024-01-24T14:16:08.724830Z"
    },
    "papermill": {
     "duration": 33.047115,
     "end_time": "2024-01-24T14:16:08.729489",
     "exception": false,
     "start_time": "2024-01-24T14:15:35.682374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/387016559.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  padded_tweets.append(torch.tensor(tweet_embedding)) # convert to tensor\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# train tensors\n",
    "\n",
    "train_padded_embeddings = padding_tweets(train_data['Text'], word2vec_greek_model)\n",
    "\n",
    "train_sentiments = label_encoder.fit_transform(train_data['Sentiment'])\n",
    "train_sentiments_tensor = torch.tensor(train_sentiments, dtype=torch.long)\n",
    "\n",
    "# valid tensors\n",
    "\n",
    "valid_padded_embeddings = padding_tweets(valid_data['Text'], word2vec_greek_model)\n",
    "\n",
    "valid_sentiments = label_encoder.fit_transform(valid_data['Sentiment'])\n",
    "valid_sentiments_tensor = torch.tensor(valid_sentiments, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5da2d",
   "metadata": {
    "papermill": {
     "duration": 0.025787,
     "end_time": "2024-01-24T14:16:08.781523",
     "exception": false,
     "start_time": "2024-01-24T14:16:08.755736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tensor Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92aa10a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:16:08.837691Z",
     "iopub.status.busy": "2024-01-24T14:16:08.837219Z",
     "iopub.status.idle": "2024-01-24T14:16:08.849679Z",
     "shell.execute_reply": "2024-01-24T14:16:08.848309Z"
    },
    "papermill": {
     "duration": 0.044343,
     "end_time": "2024-01-24T14:16:08.852273",
     "exception": false,
     "start_time": "2024-01-24T14:16:08.807930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#Transpose the first 2 dimensions of padded embeddings -> [batch_size, sequence_length, embedding_size]\n",
    "train_padded_embeddings = train_padded_embeddings.transpose(0, 1)\n",
    "valid_padded_embeddings = valid_padded_embeddings.transpose(0, 1)\n",
    "\n",
    "train_tensor_dataset = TensorDataset(train_padded_embeddings, train_sentiments_tensor)\n",
    "valid_tensor_dataset = TensorDataset(valid_padded_embeddings, valid_sentiments_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_tensor_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7291b9",
   "metadata": {
    "papermill": {
     "duration": 0.026783,
     "end_time": "2024-01-24T14:16:08.903138",
     "exception": false,
     "start_time": "2024-01-24T14:16:08.876355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sentiment RNN Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48491fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:16:09.033012Z",
     "iopub.status.busy": "2024-01-24T14:16:09.032564Z",
     "iopub.status.idle": "2024-01-24T14:16:09.047550Z",
     "shell.execute_reply": "2024-01-24T14:16:09.045685Z"
    },
    "papermill": {
     "duration": 0.120948,
     "end_time": "2024-01-24T14:16:09.050924",
     "exception": false,
     "start_time": "2024-01-24T14:16:08.929976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, cell_type, input_size, hidden_size, output_size,\n",
    "                num_layers=1, bidirectional=False, dropout=0):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        cells = {\n",
    "          \"RNN\" : nn.RNN,\n",
    "          \"LSTM\" : nn.LSTM,\n",
    "          \"GRU\" : nn.GRU\n",
    "        }\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        self.rnn = cells[cell_type](         # Pick the specific model\n",
    "            input_size=input_size,           # Number of features for each time step\n",
    "            hidden_size=hidden_size,         # rnn hidden units\n",
    "            batch_first=True,       # input & output will have batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        bidirectional_multiplier = 2 if bidirectional else 1\n",
    "        self.out = nn.Linear(hidden_size * bidirectional_multiplier, output_size) # Feed forward network\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size): final hidden state\n",
    "        # h_c shape (n_layers, batch, hidden_size): final cell state\n",
    "\n",
    "        # LSTM returns output and tuple (hidden_state, cell_state)\n",
    "        # Other RNN return output and hidden_state\n",
    "        if self.cell_type == 'LSTM':\n",
    "            r_out, (h_n, h_c) = self.rnn(x)\n",
    "        else:\n",
    "            r_out, h_n = self.rnn(x, None)\n",
    "            \n",
    "        # For bidirectional RNN, concat the final states\n",
    "        # if self.cell_type == 'LSTM':\n",
    "        if self.rnn.bidirectional:\n",
    "            h_n = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            h_n = h_n[-1,:,:]\n",
    "        # else:\n",
    "        #     # For non-LSTM, use last hidden state\n",
    "        #     h_n = h_n[-1,:,:]\n",
    "\n",
    "        # in sentiment classification, we're interested in the overall sentiment of the sequence\n",
    "        # not the sentiment at each individual time step. Therefore\n",
    "        # we use h_n (final hidden state) as the input to our linear layer (self.out)\n",
    "        out = self.out(h_n)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c21b6a",
   "metadata": {
    "papermill": {
     "duration": 0.0235,
     "end_time": "2024-01-24T14:16:09.098799",
     "exception": false,
     "start_time": "2024-01-24T14:16:09.075299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8580be6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:16:09.151380Z",
     "iopub.status.busy": "2024-01-24T14:16:09.150926Z",
     "iopub.status.idle": "2024-01-24T14:16:09.162132Z",
     "shell.execute_reply": "2024-01-24T14:16:09.160495Z"
    },
    "papermill": {
     "duration": 0.041901,
     "end_time": "2024-01-24T14:16:09.164987",
     "exception": false,
     "start_time": "2024-01-24T14:16:09.123086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_func, device):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # setting gpu\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == labels).float().mean()\n",
    "        train_acc += accuracy.item()\n",
    "\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_preds.extend(predicted.tolist())\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_train_acc = train_acc / len(train_loader)\n",
    "\n",
    "    return avg_train_loss, avg_train_acc, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb37672e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:16:09.216886Z",
     "iopub.status.busy": "2024-01-24T14:16:09.216334Z",
     "iopub.status.idle": "2024-01-24T14:16:09.226993Z",
     "shell.execute_reply": "2024-01-24T14:16:09.225671Z"
    },
    "papermill": {
     "duration": 0.040137,
     "end_time": "2024-01-24T14:16:09.229475",
     "exception": false,
     "start_time": "2024-01-24T14:16:09.189338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader, loss_func, device):\n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            # setting gpu\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            accuracy = (predicted == labels).float().mean()\n",
    "            valid_acc += accuracy.item()\n",
    "\n",
    "            all_labels.extend(labels.tolist())\n",
    "            all_preds.extend(predicted.tolist())\n",
    "    \n",
    "    avg_valid_loss = valid_loss / len(valid_loader)\n",
    "    avg_valid_acc = valid_acc / len(valid_loader)\n",
    "\n",
    "    return avg_valid_loss, avg_valid_acc, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8356af4",
   "metadata": {
    "papermill": {
     "duration": 0.024057,
     "end_time": "2024-01-24T14:16:09.279547",
     "exception": false,
     "start_time": "2024-01-24T14:16:09.255490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optuna usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c7acf7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:16:09.335783Z",
     "iopub.status.busy": "2024-01-24T14:16:09.335357Z",
     "iopub.status.idle": "2024-01-24T14:16:09.344731Z",
     "shell.execute_reply": "2024-01-24T14:16:09.343000Z"
    },
    "papermill": {
     "duration": 0.041789,
     "end_time": "2024-01-24T14:16:09.347503",
     "exception": false,
     "start_time": "2024-01-24T14:16:09.305714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import torch.optim as optim\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def objective(trial):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "#     hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n",
    "#     num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "#     if num_layers > 1:\n",
    "#         dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
    "#     else:\n",
    "#         dropout = 0\n",
    "#     cell_type = trial.suggest_categorical('cell_type', ['LSTM', 'GRU'])\n",
    "\n",
    "#     model = SentimentRNN(cell_type=cell_type, input_size=100,\n",
    "#                          hidden_size=hidden_size, output_size=3,\n",
    "#                          num_layers=num_layers, bidirectional=True,\n",
    "#                          dropout=dropout).to(device)\n",
    "    \n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#     EPOCH = 10\n",
    "\n",
    "#     train_epoch_loss, valid_epoch_loss = [], []\n",
    "#     train_epoch_acc, valid_epoch_acc = [], []\n",
    "\n",
    "#     for epoch in range(EPOCH):\n",
    "#         train_loss, train_acc, _, _ = train(model, train_loader, optimizer, loss_func, device)\n",
    "#         valid_loss, valid_acc, _, _ = evaluate(model, valid_loader, loss_func, device)\n",
    "\n",
    "#         train_epoch_loss.append(train_loss)\n",
    "#         valid_epoch_loss.append(valid_loss)\n",
    "#         train_epoch_acc.append(train_acc)\n",
    "#         valid_epoch_acc.append(valid_acc)\n",
    "\n",
    "#         print(f'Epoch {epoch+1} | Training Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "#         print(f'Epoch {epoch+1} | Validation Loss: {valid_loss:.4f}, Accuracy: {valid_acc:.4f}')\n",
    "\n",
    "#     # Combined plot for both Training and Validation Loss\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(train_epoch_loss, label='Training Loss', color='blue')\n",
    "#     plt.plot(valid_epoch_loss, label='Validation Loss', color='red')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('Training & Validation Loss Over Time')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Combined plot for both Training and Validation Accuracy\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(train_epoch_acc, label='Training Accuracy', color='orange')\n",
    "#     plt.plot(valid_epoch_acc, label='Validation Accuracy', color='green')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.title('Training & Validation Accuracy Over Time')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     avg_valid_acc = sum(valid_epoch_acc) / len(valid_epoch_acc)\n",
    "#     avg_train_acc = sum(train_epoch_acc) / len(train_epoch_acc)\n",
    "#     # returning the avg_train_acc can show how well the model was trained on the seen data, optimizing the hyperparameters based on the training accuracy\n",
    "#     # which can lead to a model that is overfitted to the training data.\n",
    "#     return avg_valid_acc\n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize') # minimize for loss\n",
    "# study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b2957",
   "metadata": {
    "papermill": {
     "duration": 0.025464,
     "end_time": "2024-01-24T14:16:09.401204",
     "exception": false,
     "start_time": "2024-01-24T14:16:09.375740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optuna Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1e8e1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:16:09.455793Z",
     "iopub.status.busy": "2024-01-24T14:16:09.455308Z",
     "iopub.status.idle": "2024-01-24T14:16:09.461484Z",
     "shell.execute_reply": "2024-01-24T14:16:09.459903Z"
    },
    "papermill": {
     "duration": 0.036649,
     "end_time": "2024-01-24T14:16:09.464052",
     "exception": false,
     "start_time": "2024-01-24T14:16:09.427403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(f\"Trial value: {trial.value}\")\n",
    "# print(\"Parameters: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "\n",
    "# for trial in study.trials:\n",
    "#     print(f\"Trial {trial.number}: Cell Type: {trial.params['cell_type']}\")\n",
    "    \n",
    "# optuna.visualization.plot_optimization_history(study)\n",
    "# optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb08163",
   "metadata": {
    "papermill": {
     "duration": 0.027323,
     "end_time": "2024-01-24T14:16:09.515532",
     "exception": false,
     "start_time": "2024-01-24T14:16:09.488209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Use the Best Hyperparameters after running Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9edff8fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:16:09.567623Z",
     "iopub.status.busy": "2024-01-24T14:16:09.567171Z",
     "iopub.status.idle": "2024-01-24T14:25:52.220013Z",
     "shell.execute_reply": "2024-01-24T14:25:52.218377Z"
    },
    "papermill": {
     "duration": 582.7095,
     "end_time": "2024-01-24T14:25:52.250622",
     "exception": false,
     "start_time": "2024-01-24T14:16:09.541122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Training Loss: 1.0955, Accuracy: 0.3594\n",
      "Epoch 2 | Training Loss: 1.0889, Accuracy: 0.3804\n",
      "Epoch 3 | Training Loss: 1.0841, Accuracy: 0.3847\n",
      "Epoch 4 | Training Loss: 1.0801, Accuracy: 0.3892\n",
      "Epoch 5 | Training Loss: 1.0764, Accuracy: 0.3948\n",
      "Epoch 6 | Training Loss: 1.0739, Accuracy: 0.3963\n",
      "Epoch 7 | Training Loss: 1.0709, Accuracy: 0.4013\n",
      "Epoch 8 | Training Loss: 1.0684, Accuracy: 0.4021\n",
      "Epoch 9 | Training Loss: 1.0648, Accuracy: 0.4049\n",
      "Epoch 10 | Training Loss: 1.0621, Accuracy: 0.4102\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# best_params = study.best_trial.params\n",
    "# best_model = SentimentRNN(cell_type=best_params['cell_type'], input_size=100,\n",
    "#                          hidden_size=best_params['hidden_size'], output_size=3,\n",
    "#                          num_layers=best_params['num_layers'], bidirectional=True,\n",
    "#                          dropout=best_params['dropout']).to(device)\n",
    "\n",
    "# optimizer = optim.Adam(best_model.parameters(), lr=best_params['lr'])\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# EPOCH = 10\n",
    "\n",
    "# train_epoch_loss, valid_epoch_loss = [], []\n",
    "# train_epoch_acc, valid_epoch_acc = [], []\n",
    "\n",
    "# for epoch in range(EPOCH):\n",
    "#         train_loss, train_acc, _, _ = train(best_model, train_loader, optimizer, loss_func, device)\n",
    "#         print(f'Epoch {epoch+1} | Training Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model = SentimentRNN(cell_type='LSTM', input_size=100,\n",
    "                         hidden_size=64, output_size=3,\n",
    "                         num_layers=2, bidirectional=True,\n",
    "                         dropout=0.24).to(device)\n",
    "\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.0005)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCH = 10\n",
    "\n",
    "train_epoch_loss, valid_epoch_loss = [], []\n",
    "train_epoch_acc, valid_epoch_acc = [], []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "        train_loss, train_acc, _, _ = train(best_model, train_loader, optimizer, loss_func, device)\n",
    "        print(f'Epoch {epoch+1} | Training Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b86a2e",
   "metadata": {
    "papermill": {
     "duration": 0.024622,
     "end_time": "2024-01-24T14:25:52.301723",
     "exception": false,
     "start_time": "2024-01-24T14:25:52.277101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34567b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T14:25:52.353801Z",
     "iopub.status.busy": "2024-01-24T14:25:52.353276Z",
     "iopub.status.idle": "2024-01-24T14:26:07.405611Z",
     "shell.execute_reply": "2024-01-24T14:26:07.404061Z"
    },
    "papermill": {
     "duration": 15.082672,
     "end_time": "2024-01-24T14:26:07.409033",
     "exception": false,
     "start_time": "2024-01-24T14:25:52.326361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "test_padded_embeddings = padding_tweets(test_data['Text'], word2vec_greek_model)\n",
    "\n",
    "# embedding in the correct shape (batch_size, sequence_length, embedding_size)\n",
    "test_padded_embeddings = test_padded_embeddings.transpose(0, 1)\n",
    "\n",
    "test_loader = DataLoader(test_padded_embeddings, batch_size=128, shuffle=False)\n",
    "\n",
    "best_model.eval()\n",
    "best_model.to(device)\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = best_model(inputs).to(device)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_predictions.extend(predicted.tolist())\n",
    "        \n",
    "predicted_sentiment_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Id': range(1, len(test_data) + 1),\n",
    "    'Predicted': predicted_sentiment_labels})\n",
    "predictions_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7351814,
     "sourceId": 66593,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 743.627489,
   "end_time": "2024-01-24T14:26:09.263921",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-24T14:13:45.636432",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
